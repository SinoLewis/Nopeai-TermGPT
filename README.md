# Nopenai-TermGPT

Following the insipiration of Sendtex Video Youtube Playlist on Generative Python Transformer, Nopenai-TermGPT aims to bring a touch of creativity to the world of Open Source AI projects. Taking cues from the insightful Sendtex Video YouTube Playlist, this repository is a manifestation of the synergy between artistic expression and the power of open collaboration.

## Objectives

### 1. Models

In the spirit of open collaboration, this module serves as a repository for trainable Language Model (LM) models. Whether it's GPT-based, BERT-based, or the innovative ChatGLM, the Models directory encapsulates the heart of the project – the brains behind the operation.

### 2. Contexts

Prompts are the essence of interaction. The Contexts module houses a collection of prompts that encapsulate user inputs. Crafting compelling and context-rich prompts is a crucial aspect of fine-tuning the AI's responses.

### 3. Queries

The Queries module hosts query functions tailored for different Language Models. Efficiently interfacing with the models ensures that the responses align with user expectations, making the terminal interaction seamless and intuitive.

### 4. Trainers

Fine-tuning models is an ongoing journey. The Trainers module is dedicated to the process of pre-training and fine-tuning newer models. Through collaborative efforts, the project aims to explore innovative approaches like PEFT (Parameter-Efficient Fine-Tuning) for quicker adaptation to various downstream applications.

### 5. UI

The UI module is the face of Nopenai-TermGPT, providing a dynamic user interface with shell access. A visually appealing and user-friendly interface enhances the overall experience of interacting with the terminal AI.

## Open Source Models

The project proudly supports a variety of Open Source Models. While GPT-based models are already integrated, the roadmap includes the incorporation of BERT-based and ChatGLM models for a diverse range of language processing capabilities.

- [x] GPT based
- [ ] BERT based
- [ ] ChatGLM

## PEFT QLoRA Fine Tuning

The introduction of Parameter-Efficient Fine-Tuning (PEFT) brings efficiency to the fine-tuning process. This library allows for the adaptation of pre-trained language models to different downstream applications without the need to fine-tune all the model's parameters. The PEFT QLoRA Fine Tuning in this project promises a quicker and more effective fine-tuning process.

## Dev Support

The Nopenai-TermGPT project thrives on community support. Pull requests are not just welcomed but encouraged. The project benefits from diverse insights and creative inputs. Consider contributing through pull requests that can enhance features such as:

1. **Web Parsing**: Integrating web parsing capabilities can open up new avenues for information retrieval and contextual responses.

2. **Subprocess**: Enhancing subprocess functionalities can contribute to the project's versatility, allowing for more intricate interactions within the terminal.

Let's champion the power of Open Source Technology together! Your creativity and expertise are integral to the evolution of Nopenai-TermGPT. Feel free to be a part of this exciting journey!

<!-- # Nopenai-TermGPT

Following the insipiration of Sendtex Video Youtube Playlist. I challenged myself to modify the creative project with a Open Source touch. The LLM's in my project are Open Source to allow pre-training & fine tuning the model. The Original project attempted to be a terminal AI prompting app which is still kept in my project objectives

## Nopenai-TermGPT modules

1. Models - Trainable LLM models or models paths can be stored here
1. Contexts - Prompts that encapsulate our input
1. Querys - Query functions based on LLM's
1. Trainers - Pre-training & fine tuning newer models
1. UI - A dynamic UI with shell access


## Open Source Models

- [x] GPT based
- [] BERT based
- [] ChatGLM

## PEFT QLoRA Fine tuning

Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model’s parameters

This Will help use fine tune various models considerably quicker according to its Algorithm

## Dev Support

Kindly feel free to post any pull request as the projects needs extensive support from your creative insights on how the will project adapt

You can might opt starting with such Pull request on Nopenai-TermGPT features such as: 

1. Web parsing
1. Subprocess

Let US Champion the power Open Source Technolgy! -->